{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX A: Data Cleaning, Data scraping, and Network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import re\n",
    "import json, urllib\n",
    "import sys\n",
    "import io\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Wikipedia pages function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(df,nodetype):\n",
    "    \"\"\" This functions Downloads the Wikipages \n",
    "    from Wikilinks column in the dataframe\n",
    "    \n",
    "    Input: dataframe and nodetype\n",
    "    -------------------------------\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    for i in range(df.shape[0]):                                             # Run through all wikilinks in dataframe \n",
    "        baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "        action = \"action=query\"\n",
    "        title = \"titles=\" + df[\"WikiLink\"].iloc[i]\n",
    "        content = \"prop=revisions&rvprop=content\"\n",
    "        dataformat =\"format=json\"\n",
    "        query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat).encode('ascii', 'ignore').decode('ascii')\n",
    "        \n",
    "        wikiresponse = urllib.request.urlopen(query)                         \n",
    "        wikidata = wikiresponse.read()                                       \n",
    "        wikitext = wikidata.decode('utf-8')\n",
    "        wikijson = json.loads(wikitext)['query']['pages']\n",
    "        key = list(wikijson.keys())[0]\n",
    "\n",
    "        if key != '-1':                                                      # If Key = '-1',the wikilink does not exist \n",
    "            wiki_Print = wikijson[key]['revisions'][0]['*']\n",
    "            sys.stdout = open(nodetype + \"_\" + df[\"WikiLink\"].iloc[i] + \".txt\", \"w\",encoding=\"utf-8\")\n",
    "            print(wiki_Print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Events dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_events_file(events):\n",
    "    \"\"\" This functions takes the .csv file --> \n",
    "    Convert it into a panda dataframe -->\n",
    "    Do some datacleaning --> \n",
    "    add the WikiLink column\n",
    "    \n",
    "    Input: 1.csv files\n",
    "    -------------------------------\n",
    "    Output: 1 dataframes\n",
    "    \"\"\"    \n",
    "    df_events = pd.read_csv(events, sep = ';', encoding = 'latin-1', dtype = object)         # Convert events.csv to panda dataframe\n",
    "    df_events['WikiLink'] = df_events['sport']+\"_at_the_2016_Summer_Olympics\"                # Add a column for WikiLinks\n",
    "    df_events['WikiLink'] = df_events['WikiLink'].str.replace('canoe', 'canoeing')           # Replace canou (for Matching Wikilink)\n",
    "    df_events['WikiLink'] = df_events['WikiLink'].str.replace('hockey', 'field hockey')      # Replace hockey (for Matching Wikilink)\n",
    "    df_events['WikiLink'] = df_events['WikiLink'].str.replace('synchronised','synchronized') # Replace synchronised (for Matching Wikilink)\n",
    "    df_events['WikiLink'] = df_events['WikiLink'].str.replace('\\s', '_')                     # Replase space with _\n",
    "    df_events = df_events.drop_duplicates(subset = [\"WikiLink\"])                             # drop duplicates\n",
    "    df_events = df_events.dropna()                                                           # Remove NanN\n",
    "    \n",
    "    return df_events\n",
    "\n",
    "df_e = read_events_file('events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download events Wikipedia pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(df_e,'events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Country dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_Country_file(countries):\n",
    "    \"\"\" This functions takes the country.csv \n",
    "    Convert it into a panda dataframe -->\n",
    "    Do some datacleaning --> \n",
    "    add the WikiLink column\n",
    "    \n",
    "    Input: .csv files\n",
    "    -------------------------------\n",
    "    Output: dataframes\n",
    "    \"\"\"\n",
    "    df_countries = pd.read_csv(countries, sep = ';',   encoding = 'latin-1', dtype = object) # Convert countries.csv into panda dataframe\n",
    "    df_countries['WikiLink'] = df_countries['country']+\"_at_the_2016_Summer_Olympics\"        # Add a column for WikiLinks\n",
    "    df_countries['country2'] = df_countries['country']                                       # Add a column country2 (Will be used for finding athletes WikiLink)\n",
    "    df_countries['country2'] = df_countries['country2'].str.replace('\\s', '_')               # Replase space with _\n",
    "    df_countries['WikiLink'] = df_countries['WikiLink'].str.replace('\\s', '_')               # Replase space with _\n",
    "    df_countries['WikiLink'] = df_countries['WikiLink'].str.replace(\"*\", \"\")                 # Remove *\n",
    "    df_countries = df_countries.dropna()                                                     # Remove NanN\n",
    "    \n",
    "    return df_countries\n",
    "\n",
    "df_c = read_Country_file('countries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Countries wikipedia pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(df_c,'countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Athletes dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_athletes_file(athletes):\n",
    "    \"\"\" This functions takes the .csv file --> \n",
    "    Convert it into a panda dataframe -->\n",
    "    Do some datacleaning \n",
    "    \n",
    "    Input: 1 .csv files\n",
    "    -------------------------------\n",
    "    Output: 1 dataframes\n",
    "    \"\"\"\n",
    "  \n",
    "    df_athletes = pd.read_csv(athletes, sep = ',',   encoding = 'utf-8', dtype = object)     # Convert athletes.csv to panda dataframe\n",
    "    df_athletes['name2'] = df_athletes['name']                                               # Add a column name2 (Will be used for finding athletes WikiLink)\n",
    "    df_athletes['name2'] = df_athletes['name2'].str.replace('\\s', '_')                       # Replase space with _\n",
    "    df_athletes = df_athletes.dropna()                                                       # Remove NanN\n",
    "                                                                                \n",
    "    \n",
    "    return df_athletes\n",
    "\n",
    "df_a = read_athletes_file('athletes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the athlete dataframe with wikilink found in the already downloaded country Wikipedia pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Athlet_WikiLink(df1, df2):\n",
    "    \"\"\" This functions finds the athletes wikilinks by \n",
    "    find all wikilinks from the coutries wikipages. \n",
    "    If the wikilinks matches a name from the athletes dataframe\n",
    "    save the wikilink in a list.\n",
    "    \n",
    "    Input: athletes dataframe, countries dataframe\n",
    "    -------------------------------\n",
    "    Output: list with athletes Wikilinks\n",
    "    \"\"\"\n",
    "    A_WikiLink = []                                                                          # Create an empty list\n",
    "    for i in df2['country2']:                                                                # Run through all countries\n",
    "        f = io.open('countries_'+ i + '_at_the_2016_Summer_Olympics.txt','r',encoding = 'utf-8').read() # Open the file\n",
    "        links = re.findall(\"\\[\\[(.*?)\\]\\]\", f)                                               # Find all links\n",
    "        links = [x.replace(' ','_') for x in links]                                          # Replase space with _\n",
    "        links = [s.split('|') for s in links]                                                # Split the links by the '|'\n",
    "\n",
    "        for j in range(len(links)):                                                          # Run through all links\n",
    "                if len(links[j]) == 1:                                                       # If the name and the links are the same\n",
    "                    if len(df1.loc[df1['name2'] == links[j][0]]) >= 1:                       # If the link excist in the athletes dataframe\n",
    "                        A_WikiLink.append([df1.at[df1.loc[df1['name2'] == links[j][0]].index[0],'id'],links[j][0]]) # append the wikilink and the 'id' number to the list\n",
    "                elif len(links[j]) == 2:                                                     # If the name and the links are not the same\n",
    "                    if len(df1.loc[df1['name2'] == links[j][1]]) >= 1:                       # If the link excist in the athletes dataframe\n",
    "                        A_WikiLink.append([df1.at[df1.loc[df1['name2'] == links[j][1]].index[0],'id'],links[j][0]]) # append the wikilink and the 'id' number to the list\n",
    "    return A_WikiLink\n",
    "\n",
    "A_WikiLink = Athlet_WikiLink(df_a,df_c)                           # Run Function\n",
    "\n",
    "df_if = pd.DataFrame(A_WikiLink, columns = ['id', 'WikiLink'])    # Convert wikilink list into panda dataframe\n",
    "df_a = df_a.merge(df_if)                                          # Merge wikilink dataframe and athletes dataframe\n",
    "df_a = df_a.drop_duplicates()                                     # Drop duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the atheletes wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(df_a,'athletes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #REDIRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Athlet_REDIRECT_WikiLink(df1):\n",
    "    \"\"\" This functions finds all athelets wikipedia pages\n",
    "    that contain '#REDIRECT' and add the correct wikilink to a list.\n",
    "    \n",
    "    Input: athletes dataframe\n",
    "    -------------------------------\n",
    "    Output: list with correct athletes Wikilinks\n",
    "    \"\"\"\n",
    "    RE_WikiLink = []                                                                          # Create an empty list\n",
    "    for i in df1['WikiLink']:                                                                 # Run through all atheletes\n",
    "        f = io.open('athletes_'+ i + '.txt','r',encoding = 'utf-8').read()                    # Open the file\n",
    "        if len(re.findall(\"#REDIRECT\", f)) > 0:                                               # If #REDIRECT\n",
    "            links = re.findall(\"\\[\\[(.*?)\\]\\]\", f)                                            # Find the new links\n",
    "            links = [x.replace(' ','_') for x in links]                                       # Replase space with _\n",
    "            \n",
    "            for j in range(len(links)):                                                       # Run through all links                                                     # If the name and the links are the same\n",
    "                if len(df1.loc[df1['WikiLink'] == i]) >= 1:                                   # If the link excist in the athletes dataframe\n",
    "                        RE_WikiLink.append([df1.at[df1.loc[df1['WikiLink'] == i].index[0],'id'], links[j]]) # append the wikilink and the 'id' number to the list\n",
    "               \n",
    "    return RE_WikiLink\n",
    "\n",
    "RE_WikiLink = Athlet_REDIRECT_WikiLink(df_a)                         # Run Function\n",
    "df_if = pd.DataFrame(RE_WikiLink, columns = ['id', 'RE_WikiLink'])   # Convert wikilink list into panda dataframe\n",
    "df_a_RE = df_a.merge(df_if)                                          # Merge wikilink dataframe and athletes dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the REDIRECTED Wikipedia pages cannot be downloaded due to f.eks. the name consists of an 'é'. Therefore these 44 athletes will be removed from the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove #REDIRECT from dataframe:\n",
    "df_a = df_a[~df_a.id.isin(df_a_RE.id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Olympic Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create a networkx graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLinksAndRemove(f,df1,df2,df3):\n",
    "    \"\"\" Helperfunction to find links in wikipages \n",
    "    and remove the link if it is not a wikilink \n",
    "    in the dataframes.\n",
    "    \n",
    "    Input: the file, and the 3 dataframes\n",
    "    -------------------------------\n",
    "    Output: list of edges\n",
    "    \"\"\"\n",
    "    links = re.findall(\"\\[\\[(.*?)\\]\\]\", f)                         # Use a regular expression to extract all outgoing links\n",
    "    links = [x.replace(' ','_') for x in links]                    # Replace space with _\n",
    "    links = [s.split('|') for s in links]                          # Split the links by the '|'\n",
    "    \n",
    "    \n",
    "    edges = []                                                     # An empty list for edges\n",
    "    for i in range(len(links)):                                    # Run through all links\n",
    "                                                                   # For each link, check if the target is in the data.\n",
    "        if len(df1.loc[df1['WikiLink'] == links[i][0]]) >= 1 or len(df2.loc[df2['WikiLink'] == links[i][0]]) >= 1 or len(df3.loc[df3['WikiLink'] == links[i][0]]) >= 1:\n",
    "            edges.append(links[i][0])                              # If yes add the link to the edge list. If no, discard it.\n",
    "    return(edges)\n",
    "\n",
    "\n",
    "def AddNodes(G, df, nodetype):\n",
    "    \"\"\" Function to add nodes to the graph. \n",
    "    Every wikilink in the data is a node in the graph.\n",
    "     \n",
    "    Input: The NetworkX DiGraph, The dataframe and the nodetype\n",
    "    -------------------------------\n",
    "    Output: The NetworkX DiGraph\n",
    "    \"\"\"\n",
    "    for i in range(df.shape[0]):                                  # Run through all wikilinks in the dataframe\n",
    "        G.add_node(df.WikiLink.iloc[i], nodetype = nodetype)      # Add the node to the Graph\n",
    " \n",
    "   \n",
    "def AddEdges(G, df1, df2, df3, nodetype):\n",
    "    \"\"\" Function to add edges to the graph. \n",
    "     \n",
    "    Input: The NetworkX DiGraph, all dataframe and the nodetype\n",
    "    -------------------------------\n",
    "    Output: The NetworkX DiGraph\n",
    "    \"\"\"\n",
    "    for i in range(df1.shape[0]):                                 # Run through all wikilinks in the dataframe                                                                  \n",
    "        Node = df1['WikiLink'].iloc[i]                            # Open the page file\n",
    "        f = io.open(nodetype + Node + \".txt\",'r',encoding = 'utf-8').read()\n",
    "        edgesTo = findLinksAndRemove(f,df1,df2,df3)               # Run the helperfuncktion to find links and remove the link if it is not in the data.\n",
    "        \n",
    "        for j in edgesTo:                                         # Run through all the finded edges\n",
    "            if j in list(G.nodes):                                # If the edge link to a node add edge to Graph\n",
    "                G.add_edge(Node, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a NetworkX DiGraph to store the network. Store also the properties of the nodes (i.e. from which dataframe they hail).\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add Nodes\n",
    "AddNodes(G, df_c, 'countries')\n",
    "AddNodes(G, df_e, 'sports')\n",
    "AddNodes(G, df_a, 'athletes')\n",
    "\n",
    "# Add edges\n",
    "AddEdges(G, df_c, df_e, df_a, 'countries_')\n",
    "AddEdges(G, df_e, df_c, df_a, 'events_')\n",
    "AddEdges(G, df_a, df_e, df_c, 'athletes_')\n",
    "\n",
    "# Check if nodes do not have any out- or in- degrees. These may discard from the network.\n",
    "remove = [node for node, degree in dict(G.degree()).items() if degree == 0]\n",
    "G.remove_nodes_from(remove) \n",
    "\n",
    "# Find largest connected_components\n",
    "largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "# Add dataframes, ass attributes to the nodes\n",
    "node_attr_e = df_e.set_index('WikiLink').to_dict('index')\n",
    "node_attr_c = df_c.set_index('WikiLink').to_dict('index')\n",
    "node_attr_a = df_a.set_index('WikiLink').to_dict('index')\n",
    "nx.set_node_attributes(G, node_attr_e)\n",
    "nx.set_node_attributes(G, node_attr_c)\n",
    "nx.set_node_attributes(G, node_attr_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataframes and Network\n",
    "Finally, save the 3 cleaned dataframes and the network for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframes:\n",
    "pickle.dump(df_a, open('df_athletes.txt', 'wb'))\n",
    "pickle.dump(df_c, open('df_countries.txt', 'wb'))\n",
    "pickle.dump(df_e, open('df_events.txt', 'wb'))\n",
    "\n",
    "# Save the network\n",
    "pickle.dump(G, open('G.txt', 'wb'))\n",
    "\n",
    "# Save as the graph as .gexf for making an interactive graph plot in GEPHI with sigma js exporter. \n",
    "nx.write_gexf(G, \"G.gexf\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
